{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a21d3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5daea339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 06:46:24.224962: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-16 06:46:24.225007: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install SentencePiece\n",
    "# !pip install transformers\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d17b8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9f3b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_root = \"/home/jbobro/final_project/W266_Final_Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b27e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(df_name):\n",
    "    return pd.read_parquet(f\"{my_root}/Processed_Data/{df_name}.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f167d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract = pull_data(\"df_abstract\")\n",
    "df_author = pull_data(\"df_author\")\n",
    "df_title = pull_data(\"df_title\")\n",
    "df_fid = pull_data(\"df_fid\")\n",
    "df_bow = pull_data(\"df_bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389703a9",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "Text generation models are those where an input is given, and the model generates the next sequence that is contextually and gramatically correct. We propose that, given our collection of succesful abstracts, we can train such a model (potentially on top of an already pre-trained model) to generate a succesful abstract that may be approved by the NSF. \n",
    "\n",
    "\n",
    "#### Common Metrics of evaluation:\n",
    "  - Cross Entropy Loss\n",
    "  - Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48e360",
   "metadata": {},
   "source": [
    "## GPT-2 Text Generation Model - Decoder Only\n",
    "Can we generate a meaningful abstract given a sentence or two? \n",
    "\n",
    "1. Identify evaluation task (GLUE) \n",
    "2. Determine pre-training approaches\n",
    "3. Determine pre-training data sets \n",
    "4. Determine fine-tuning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e6df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 05:08:22.857792: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-16 05:08:22.857849: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-16 05:08:22.857875: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-3): /proc/driver/nvidia/version does not exist\n",
      "2022-11-16 05:08:22.884273: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-16 05:08:23.212776: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "# generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dad7dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sample = df_abstract['abstract'][1]\n",
    "first_sentence = abs_sample[20:397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7098f7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 71), dtype=int32, numpy=\n",
       "array([[40127,   615,   602,   416,  1583,    13, 12246, 22553,   290,\n",
       "          465,  7810,   379,   262,  2524,   286, 33901, 10599, 14783,\n",
       "           12,    32,   316,   482,  2787, 39369,    11,  5140,   319,\n",
       "          262,  9084, 10599, 14783, 24078,   286,    82,  4927, 27885,\n",
       "           11,   423,  9257,  6655,  1111, 38967,   290,  8482,  9251,\n",
       "           11,   780,   484,  1283,   284,  7603,   262,   763,    12,\n",
       "        13966, 33928,   286, 40205,   290, 12972,    70,  1820,  5107,\n",
       "          286,  1111, 18568, 43372, 25509,   290, 20950,   220]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_start = 'Excavations by Dr. Alan Simmons and his colleagues at the site ofAkrotiri-Aetokremnos, located on the Akrotiri Peninsula ofsouthern Cyprus, have greatly surprised both archaeologists andbiologists, because they seem to indicate the co-occurrence ofhumans and pygmy forms of both hippopotamus and elephant '\n",
    "input_ids = gpt2_tokenizer.encode(text_start, return_tensors='tf')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666d3d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excavations by Dr. Alan Simmons and his colleagues at the site ofAkrotiri-Aetokremnos, located on the Akrotiri Peninsula ofsouthern Cyprus, have greatly surprised both archaeologists andbiologists, because they seem to indicate the co-occurrence ofhumans and pygmy forms of both hippopotamus and elephant.  Datedto the ninth millennium BC, this stratified cave site containsboth stone tools as well as the broken bones of both species.These data also indicate that human settlement of the easternMediterranean islands occurred ca. 2,000 years earlier thanpreviously expected.  Evidence seems to indicate that a trueassociation exists between the bones and stone tools.  Thisimplies human hunting of these species and a probable human rolein their extinction.  In this project, Dr. Simmons and hiscolleagues will analyze the materials recovered during multipleseasons of field research.  Objectives include analysis of faunaland material culture assemblages, of site geomorphology,stratigraphy and paleoenvironment, as well as refinement of thechronological sequence.In many parts of the world the disappearance of many species ofanimals appears to be associated with human factors.Archaeologists and biologists have, for many years, argued aboutthe human role in the extinction of such large species asmammoths and mastodons in North America.  What makes the issuecomplicated is that these faunal changes occurred at the end ofthe last \"Ice Age,\" and it is difficult to separate out thepotential climatic factor.  Therefore, researchers have turnedtheir attention to other areas of the world.  It is in thiscontext that the Cyprus case is of primary interest.  Throughstudy of the cave sediments insight will be gained into climaticchange - or stability.  Analysis of bone breakage patterns willindicate how humans utilized and possibly killed these animals.Given the serious problem of species loss today, this research isimportant because it will set the issue into a broaderchronological context.  It will also increase our understandingof early Mediterranean prehistory and help to document expandinghuman capabilities there.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "110edc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: Excavations by Dr. Alan Simmons and his colleagues at the site ofAkrotiri-Aetokremnos, located on the Akrotiri Peninsula ofsouthern Cyprus, have greatly surprised both archaeologists andbiologists, because they seem to indicate the co-occurrence ofhumans and pygmy forms of both hippopotamus and elephant ichthyosaurs in our own day - a discovery that has led many researchers looking for some kind's proof as well as an explanation about how their ancestors evolved from those bipedal animals!\n",
      "If you want more details then see... A link is posted between these three accounts: http://www'thenkionaelden/aeon105057011143_2x2049371430861/-fv3e60ea0cf4b42 https:/ / twitter & facebook The following text was first published here under permission granted [with attribution]\n"
     ]
    }
   ],
   "source": [
    "generated_text_outputs = gpt2_model.generate(\n",
    "    input_ids, \n",
    "    max_length=300,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.5,\n",
    "    top_p=0.92,\n",
    "    temperature=.85,\n",
    "    do_sample=True,\n",
    "    top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Clearly the raw GPT-2 model is not doing too well at predicting Scientific abstracts\n",
    "for i, beam in enumerate(generated_text_outputs):\n",
    "  print()\n",
    "  print(\"{}: {}\".format(i, gpt2_tokenizer.decode(beam, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e6f80",
   "metadata": {},
   "source": [
    "## GPT-3\n",
    "\n",
    "\"While Generative Pre-Trained Transformers are a great milestone in the artificial intelligence race, it’s not equipped to handle complex and long language formations. If you imagine a sentence or paragraph that contains words from very specialized fields such as literature, finance or medicine, for example, the model would not be able to generate appropriate responses without sufficient training beforehand.\"\n",
    "  - This means without sufficient compute GPT-3 and GPT-2 would be difficult for us to train\n",
    "  - It will most likely not give us as topic-specific accurate results (more focused on language structure than niche corpus) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sample = df_abstract['abstract'][3]\n",
    "first_sentence = abs_sample[20:397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b66072",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Who is Elon Musk'\n",
    "abs_sample = df_abstract['abstract'][1\n",
    "result = generator(text, max_length=100, do_sample=True, temperature=0.9)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971eea77",
   "metadata": {},
   "source": [
    "## T5 model for text generation\n",
    "*Transformer: type of neural network architectu*\n",
    "\n",
    "How does T5 work? \n",
    "- T5 is trained on C4 -- Colossal Clean Crawled Corpus (obtained by scraping web pages and ignoring the markup from the HTML, hence causing lots of gibberish text as we see bellow, to flow through to the model \n",
    "\n",
    "\n",
    "**Reference Abstract: (Not fed into the model)**\n",
    "\n",
    "\"Recent advances in our understanding of minor biologicalfractionation of Ge/Si by oceanic diatoms suggest that Ge/Si)opal  variationsmeasured in late Pleistocene piston cores and Cenozoic  drill cores arerecording whole ocean (Ge/Si)seawater variations  driven by rapid and largeglacial to interglacial changes in  continental weathering intensity and riverfluxes to the sea. If  so, then it is clearly important to produce highresolution records  of (Ge/Si)opal in cores with well-established  18 O and 14Cstratigraphies across transitions to establish the shape and timing  of theoceanic Ge/Si response..\" \n",
    "\n",
    "**Input Sentence:**\n",
    "\n",
    "\" Recent advances in our understanding of minor biologicalfractionation of Ge/Si by oceanic diatoms suggest that Ge/Si)opal  variationsmeasured in late Pleistocene piston cores and Cenozoic  drill cores arerecording whole ocean (Ge/Si)seawater variations  driven by rapid and largeglacial to interglacial changes in  continental weathering intensity and riverfluxes to the sea.\"\n",
    "\n",
    "**Pre-Trained T5 model summarization**\n",
    "\n",
    "\"this project aims to produce high resolution records of (Ge/Si)opal in cores with well-established 18 O and 14Cstratigraphies across transitions . it is also necessary to carefully evaluate and remove the effects of ge/si fractionation in local records, says dr. j. floelich. this is aimed at accomplishingtwo major goals: (1) Produce highresolution records incores that contain abundant diatoms and forams in collaboration with others' measurements a -\"\n",
    "\n",
    "Notice the previously unseen output that matches between the input sentence, and the predicted output: \n",
    "- Original: If  so, then it is clearly important to produce highresolution records  of (Ge/Si)opal ..\n",
    "- Predicted: Produce highresolution records incores that contain abundant diatoms and forams in collaboration with others' measurements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 model,\n",
    "                 n_examples,\n",
    "                 data_filename,\n",
    "                 max_length=128,\n",
    "                 batch_size=16,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.n_examples = n_examples\n",
    "        self.data_filename = data_filename\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        # Initialize row order, call on_epoch_end to shuffle row indices\n",
    "        self.row_order = np.arange(1, self.n_examples+1)\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_examples // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_start = idx * self.batch_size\n",
    "        batch_end = (idx + 1) * self.batch_size\n",
    "\n",
    "        # Indices to skip are the ones in the shuffled row_order before and\n",
    "        # after the chunk we'll use for this batch\n",
    "        batch_idx_skip = self.row_order[:batch_start] + self.row_order[batch_end:]\n",
    "        df = pd.read_csv(self.data_filename, skiprows=batch_idx_skip)\n",
    "        \n",
    "        text_pairs = df[['orig', 'target']].values.astype(str).tolist()\n",
    "        \n",
    "        batch_data = preprocess_data(\n",
    "            text_pairs,\n",
    "            self.tokenizer,\n",
    "            self.model,\n",
    "            self.max_length\n",
    "        )\n",
    "\n",
    "        return batch_data\n",
    "    \n",
    "    def __call__(self):\n",
    "        for i in range(self.__len__()):\n",
    "            yield self.__getitem__(i)\n",
    "            \n",
    "            if i == self.__len__()-1:\n",
    "                self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.row_order = list(np.random.permutation(self.row_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da2c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937e34dcb28949399e924d0562474891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a6132c27ff46b7be985e72244ed3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbobro/anaconda3/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35d97524bed4ac985d08796426e0343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 06:46:59.188441: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-16 06:46:59.188491: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-16 06:46:59.188514: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-3): /proc/driver/nvidia/version does not exist\n",
      "2022-11-16 06:46:59.214537: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-16 06:46:59.529066: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-base'\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "t5_model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "# t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "# t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# t5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf3928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Professor Hull is engaged in an investigation of two separateprojects:  the first concerns criteria for selection of taxonomicprinciples; the second concerns the processes by which scienceitself develops. '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_sample = df_abstract['abstract'][3]\n",
    "first_sentence = abs_sample[0:205]\n",
    "first_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618cf734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Professor Hull is engaged in an investigation of two separateprojects:  the first concerns criteria for selection of taxonomicprinciples; the second concerns the processes by which scienceitself develops.  While seemingly unrelated, Professor Hull isusing the first study as a case for exemplifying his argumentsconcerning the processes of scientific development.Although the distance between alternative principles of biologicalclassification and anything that might be considered \"evidence\" forthem is great, empirical considerations can be brought to bear onthe decision to choose one set of principles of classification overothers.  Professor Hull argues that the taxonomic principles thatresult in classifications that facilitate the discovery of naturalregularities are preferable to those that do not facilitate suchdiscoveries.  Dr Hull is examining the recent literature inevolutionary biology to see what sorts of classification aid eitherphylogenetic reconstruction or the discovery of regularities in theevolutionary process.  The results of these investigations will inturn be used to support the contention that the use that scientistsmake of each others work is central to the ongoing process ofscience.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3422a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = first_sentence\n",
    "t5_input_text = \"summarize: \" + abs_sample\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6459f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 06:48:59.453579: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x55d1b9819d50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-16 06:48:59.453618: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Host, Default Version\n",
      "2022-11-16 06:49:00.483488: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-16 06:49:02.012898: I tensorflow/compiler/jit/xla_compilation_cache.cc:399] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=7, \n",
    "                                   no_repeat_ngram_size=2, \n",
    "                                   min_length=400, \n",
    "                                   max_length=600)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False)\n",
    "       for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b91ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
