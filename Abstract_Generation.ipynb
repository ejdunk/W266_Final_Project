{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6a21d3",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5daea339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install SentencePiece\n",
    "# !pip install transformers\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d17b8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f3b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_root = \"/home/jbobro/final_project/W266_Final_Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b27e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(df_name):\n",
    "    return pd.read_parquet(f\"{my_root}/Processed_Data/{df_name}.parquet.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f167d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract = pull_data(\"df_abstract\")\n",
    "df_author = pull_data(\"df_author\")\n",
    "df_title = pull_data(\"df_title\")\n",
    "df_fid = pull_data(\"df_fid\")\n",
    "df_bow = pull_data(\"df_bow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48e360",
   "metadata": {},
   "source": [
    "## GPT-2 Text Generation Model - Decoder Only\n",
    "Can we generate a meaningful abstract given a sentence or two? \n",
    "\n",
    "1. Identify evaluation task (GLUE) \n",
    "2. Determine pre-training approaches\n",
    "3. Determine pre-training data sets \n",
    "4. Determine fine-tuning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193df282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_model = TFGPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sample = df_abstract['abstract'][1]\n",
    "first_sentence = abs_sample[20:397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_start = 'This award is for one year of support to continue work onsamples '\n",
    "input_ids = gpt2_tokenizer.encode(text_start, return_tensors='tf')\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110edc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_outputs = gpt2_model.generate(\n",
    "    input_ids, \n",
    "    max_length=300,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=1.5,\n",
    "    top_p=0.92,\n",
    "    temperature=.85,\n",
    "    do_sample=True,\n",
    "    top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Clearly the raw GPT-2 model is not doing too well at predicting Scientific abstracts\n",
    "for i, beam in enumerate(generated_text_outputs):\n",
    "  print()\n",
    "  print(\"{}: {}\".format(i, gpt2_tokenizer.decode(beam, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49e6f80",
   "metadata": {},
   "source": [
    "## GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8f8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sample = df_abstract['abstract'][3]\n",
    "first_sentence = abs_sample[20:397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b66072",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Who is Elon Musk'\n",
    "abs_sample = df_abstract['abstract'][1\n",
    "result = generator(text, max_length=100, do_sample=True, temperature=0.9)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971eea77",
   "metadata": {},
   "source": [
    "## T5 model for text generation\n",
    "*Transformer: type of neural network architectu*\n",
    "\n",
    "How does T5 work? \n",
    "- T5 is trained on C4 -- Colossal Clean Crawled Corpus (obtained by scraping web pages and ignoring the markup from the HTML, hence causing lots of gibberish text as we see bellow, to flow through to the model \n",
    "\n",
    "\n",
    "**Reference Abstract: (Not fed into the model)**\n",
    "\n",
    "\"Recent advances in our understanding of minor biologicalfractionation of Ge/Si by oceanic diatoms suggest that Ge/Si)opal  variationsmeasured in late Pleistocene piston cores and Cenozoic  drill cores arerecording whole ocean (Ge/Si)seawater variations  driven by rapid and largeglacial to interglacial changes in  continental weathering intensity and riverfluxes to the sea. If  so, then it is clearly important to produce highresolution records  of (Ge/Si)opal in cores with well-established  18 O and 14Cstratigraphies across transitions to establish the shape and timing  of theoceanic Ge/Si response..\" \n",
    "\n",
    "**Input Sentence:**\n",
    "\n",
    "\" Recent advances in our understanding of minor biologicalfractionation of Ge/Si by oceanic diatoms suggest that Ge/Si)opal  variationsmeasured in late Pleistocene piston cores and Cenozoic  drill cores arerecording whole ocean (Ge/Si)seawater variations  driven by rapid and largeglacial to interglacial changes in  continental weathering intensity and riverfluxes to the sea.\"\n",
    "\n",
    "**Pre-Trained T5 model summarization**\n",
    "\n",
    "\"this project aims to produce high resolution records of (Ge/Si)opal in cores with well-established 18 O and 14Cstratigraphies across transitions . it is also necessary to carefully evaluate and remove the effects of ge/si fractionation in local records, says dr. j. floelich. this is aimed at accomplishingtwo major goals: (1) Produce highresolution records incores that contain abundant diatoms and forams in collaboration with others' measurements a -\"\n",
    "\n",
    "Notice the previously unseen output that matches between the input sentence, and the predicted output: \n",
    "- Original: If  so, then it is clearly important to produce highresolution records  of (Ge/Si)opal ..\n",
    "- Predicted: Produce highresolution records incores that contain abundant diatoms and forams in collaboration with others' measurements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af886944e3df481fa7d3ea9e3f1aba76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf624333ede5400487f5f225e7ed3f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 04:46:59.853424: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-16 04:46:59.853529: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-16 04:46:59.853561: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (instance-3): /proc/driver/nvidia/version does not exist\n",
      "2022-11-16 04:46:59.955307: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-16 04:47:00.433167: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-11-16 04:47:02.606646: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 131596288 exceeds 10% of free system memory.\n",
      "2022-11-16 04:47:03.552542: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 131596288 exceeds 10% of free system memory.\n",
      "2022-11-16 04:47:03.719214: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 131596288 exceeds 10% of free system memory.\n",
      "2022-11-16 04:47:05.623267: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n",
      "2022-11-16 04:47:05.750479: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16777216 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_sample = df_abstract['abstract'][3]\n",
    "first_sentence = abs_sample[20:397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3422a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = first_sentence\n",
    "t5_input_text = \"summarize: \" + abs_sample\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6459f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=7, \n",
    "                                   no_repeat_ngram_size=2, \n",
    "                                   min_length=400, \n",
    "                                   max_length=600)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False)\n",
    "       for g in t5_summary_ids])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
