{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ce6cf4",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af90f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f21ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98d19881",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_root = \"/home/jbobro/final_project/W266_Final_Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This array will hold all raw abstracts (w/ no association to year etc)\n",
    "abstracts = []\n",
    "file_key = []\n",
    "rootdir = f\"{my_root}/Data/ABSTRACT_DATA/\"\n",
    "latest_file = ''\n",
    "\n",
    "# Loop through directories to grab all of the files\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for filename in files:\n",
    "        f = os.path.join(subdir, filename)\n",
    "        # Open the file (w/ latin1 encoding to forgive speical characters)\n",
    "        with open(f, 'r', encoding='latin1') as file:\n",
    "            # grab file key that we will use to join with Bag of Words\n",
    "            file_key.append(f[-12:][:-4])\n",
    "            # Create abstract variable to hold abstract after we parse the file\n",
    "            abstract = []\n",
    "            all_text = file.readlines()\n",
    "            # Cut out white space\n",
    "            output = [line.strip() for line in all_text]\n",
    "            # Isolate abstract by parsing the file and grabbing all text only after the \"Abstract\" section\n",
    "            is_abstract = False\n",
    "            for val in output:\n",
    "                if is_abstract:\n",
    "                    abstract.append(val)\n",
    "                if val.startswith('Abstract'):\n",
    "                      is_abstract = True\n",
    "            abstracts.append(''.join(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d8a82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ids_auth = []\n",
    "file_authors = []\n",
    "\n",
    "file_ids_title = []\n",
    "file_titles = []\n",
    "\n",
    "file_ids_fid = []\n",
    "file_key_fid = []\n",
    "\n",
    "bow_file_id = []\n",
    "bow_file_word = []\n",
    "bow_file_word_count = []\n",
    "root_dir_nsf = f\"{my_root}/Data/NSF_DATA/\"\n",
    "for subdir, dirs, files in os.walk(root_dir_nsf):\n",
    "    for filename in files:\n",
    "        f = os.path.join(subdir, filename)\n",
    "        # Process authors \n",
    "        if f[-12:] == 'docauths.txt':\n",
    "          with open(f, 'r', encoding='latin1') as file:\n",
    "            docauths = file.readlines()\n",
    "          for row in docauths:\n",
    "            auth_data = row.split('\\t')\n",
    "            file_ids_auth.append(int(auth_data[0]))\n",
    "            file_authors.append(auth_data[1].strip())\n",
    "\n",
    "        # Process titles\n",
    "        if f[-13:] == 'doctitles.txt':\n",
    "          with open(f, 'r', encoding='latin1') as file:\n",
    "            doctitles = file.readlines()\n",
    "          for row in doctitles:\n",
    "            title_data = row.split('\\t')\n",
    "            file_ids_title.append(int(title_data[0]))\n",
    "            file_titles.append(title_data[1].strip())\n",
    "        \n",
    "        # Process idnsfid \n",
    "        if f[-11:] == 'idnsfid.txt':\n",
    "          with open(f, 'r', encoding='latin1') as file:\n",
    "            docfids = file.readlines()\n",
    "          for row in docfids:\n",
    "            fids_data = row.split('\\t')\n",
    "            file_ids_fid.append(int(fids_data[0]))\n",
    "            file_key_fid.append(fids_data[1].strip())\n",
    "        \n",
    "        # Process BOW data \n",
    "        if f[-12:] == 'docwords.txt':\n",
    "          with open(f, 'r', encoding='latin1') as file:\n",
    "            docwords = file.readlines()\n",
    "          for row in docwords:\n",
    "            words_data = row.split(' ')\n",
    "            bow_file_id.append(int(words_data[0]))\n",
    "            bow_file_word.append(int(words_data[1].strip()))\n",
    "            bow_file_word_count.append(int(words_data[2].strip()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9a54373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of abstract df: 134616\n",
      "Length of author df: 199475\n",
      "Length of title df: 128818\n",
      "Length of fid df: 128818\n",
      "Length of BOW df: 10449902\n"
     ]
    }
   ],
   "source": [
    "data = {'file_key':file_key, 'abstract': abstracts}\n",
    "df_abstract = pd.DataFrame(data)\n",
    "\n",
    "data_author = {'file_id':file_ids_auth, 'file_author': file_authors}\n",
    "df_author = pd.DataFrame(data_author)\n",
    "\n",
    "data_title = {'file_id':file_ids_title, 'file_title': file_titles}\n",
    "df_title = pd.DataFrame(data_title)\n",
    "\n",
    "data_fids = {'file_id':file_ids_fid, 'file_key': file_key_fid}\n",
    "df_fid = pd.DataFrame(data_fids)\n",
    "\n",
    "data_bow = {'file_id':bow_file_id, 'word_id': bow_file_word, 'word_freq': bow_file_word_count}\n",
    "df_bow = pd.DataFrame(data_bow)\n",
    "\n",
    "print(f\"Length of abstract df: {len(df_abstract)}\")\n",
    "print(f\"Length of author df: {len(df_author)}\")\n",
    "print(f\"Length of title df: {len(df_title)}\")\n",
    "print(f\"Length of fid df: {len(df_fid)}\")\n",
    "print(f\"Length of BOW df: {len(df_bow)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8006e93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excavations by Dr. Alan Simmons and his colleagues at the site ofAkrotiri-Aetokremnos, located on the Akrotiri Peninsula ofsouthern Cyprus, have greatly surprised both archaeologists andbiologists, because they seem to indicate the co-occurrence ofhumans and pygmy forms of both hippopotamus and elephant.  Datedto the ninth millennium BC, this stratified cave site containsboth stone tools as well as the broken bones of both species.These data also indicate that human settlement of the easternMediterranean islands occurred ca. 2,000 years earlier thanpreviously expected.  Evidence seems to indicate that a trueassociation exists between the bones and stone tools.  Thisimplies human hunting of these species and a probable human rolein their extinction.  In this project, Dr. Simmons and hiscolleagues will analyze the materials recovered during multipleseasons of field research.  Objectives include analysis of faunaland material culture assemblages, of site geomorphology,stratigraphy and paleoenvironment, as well as refinement of thechronological sequence.In many parts of the world the disappearance of many species ofanimals appears to be associated with human factors.Archaeologists and biologists have, for many years, argued aboutthe human role in the extinction of such large species asmammoths and mastodons in North America.  What makes the issuecomplicated is that these faunal changes occurred at the end ofthe last \"Ice Age,\" and it is difficult to separate out thepotential climatic factor.  Therefore, researchers have turnedtheir attention to other areas of the world.  It is in thiscontext that the Cyprus case is of primary interest.  Throughstudy of the cave sediments insight will be gained into climaticchange - or stability.  Analysis of bone breakage patterns willindicate how humans utilized and possibly killed these animals.Given the serious problem of species loss today, this research isimportant because it will set the issue into a broaderchronological context.  It will also increase our understandingof early Mediterranean prehistory and help to document expandinghuman capabilities there.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_abstract['abstract'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdfe2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_data(df, df_name):\n",
    "    df.to_parquet(f\"{my_root}/Processed_Data/{df_name}.parquet.gzip\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0966d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_data(df_abstract, \"df_abstract\")\n",
    "push_data(df_author, \"df_author\")\n",
    "push_data(df_title, \"df_title\")\n",
    "push_data(df_fid, \"df_fid\")\n",
    "push_data(df_bow, \"df_bow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
